<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Jinbin Bai Homepage">
    <meta name="author" content="Jinbin Bai">
    <link rel="shortcut icon" href="images/favicon.ico" />
    <meta name="google-site-verification" content="Befm1EhO7nNT_5JmVbLWxjiqeoQRXSUluMPi84niqI4" />
    <!-- <script>
                var browserLanguage = navigator.language || navigator.userLanguage;
                if (browserLanguage.toLowerCase() === 'zh-cn') {
                    window.location.href = 'https://www.baidu.com'; 
                }
            </script> -->
    <title>Jinbin Bai Homepage</title>

    <style>
        /* 应用Comic Sans MS字体到页面中所有文本 */
        body {
            /* 使用Comic Sans MS字体，如果无法加载则使用默认字体 */
            font-family: "Comic Sans MS", sans-serif;

        }

        /* 也可以为特定元素设置字体 */
        h1,
        h2,
        p {
            font-family: "Comic Sans MS", sans-serif;
        }

        /* 其他CSS样式... */
    </style>

</head>

<body>
    <table
        style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tr style="padding:0px">
                            <td style="padding:2.5%;width: 60%;vertical-align:middle">
                                <h1 style="text-align: center;">Jinbin Bai</h1>

                                <p>
                                    I received B.S. in Computer Science from Nanjing University and high school diploma from
                                    the Affiliated High School of Shanxi University. After that, I studied at CS Dept.
                                    of National University of Singapore and founded <a
                                        href="https://huggingface.co/MeissonFlow">MeissonFlow Research</a> (See Organization Card for more details) for
                                    developing masking paradigm in generative modeling. I have been working with Prof. Shuicheng Yan and Prof. Ming-Hsuan Yang.
                                </p>
                                <p>
                                    I am trying to find ways to build interactive models and algorithms for content
                                    creation. I want to build the world with visual prior, though i sadly agree that the
                                    language prior dominates current unified models. I love imagination, I love
                                    Astronomy. So I made an analogy in the figure below.
                                </p>




                                <!-- <p> -->
                                <!-- Email: jinbin5bai@gmail.com; jinbin.bai@u.nus.edu; jinbinb@acm.org -->
                                <!-- And I am always looking for Ph.D. opportunities. -->
                                <!-- </p> -->
                                <p style="text-align: center;">
                                    <a href="mailto:jinbin5bai@gmail.com">Email</a> &nbsp;/&nbsp;
                                    <!-- <a href="https://www.linkedin.com/in/jinbin-bai">Linkedin</a>&nbsp;/&nbsp; -->
                                    <a href="https://scholar.google.com/citations?hl=en&user=PAfNNrYAAAAJ">Google
                                        Scholar</a>&nbsp;/&nbsp;
                                    <a href="https://github.com/viiika">Github</a> &nbsp;/&nbsp;
                                    <a href="https://huggingface.co/BryanW">Hugging Face</a> &nbsp;/&nbsp;
                                    <!-- <a href="./VikaResearch.html">Vika Research</a> &nbsp;&nbsp;/&nbsp; -->
                                    <a href="https://x.com/Jinbin_Bai">X</a> &nbsp;
                                    <!-- /&nbsp; -->
                                    <!-- <a href="https://discord.gg/6B8XF58atz">Discord</a>  -->
                                    <!-- <a href="bio.txt">Short Bio</a> -->
                                </p>
                            </td>
                            <td style="padding:2.5%;width:25%;max-width:25%">
                                <img style="width:60%;max-width:100%" alt="profile photo"
                                    src="images/Jinbin_profile_new.jpg">
                                <p>
                                    Jinbin in Cambridge, UK, 2025.
                                </p>

                            </td>
                        </tr>
        </tbody>
    </table>


    <div style="text-align: center;">
        <img src="paper/gravity.jpeg" alt="Research"
            style="width: 100%; max-width: 80%; margin: 8px auto; display: block;">

        <p style="text-align: center; line-height: 1.6;">
            <a href="" style="display: block;">Universal gravity pulls matter into wells.</a>
            <a href="" style="display: block;">Gradient descent pulls models into minima.</a>
            <a href="" style="display: block;">Optimization landscapes are just the gravity wells of learning.</a>
            <a href="" style="display: block;">Some minima (like Mercury) are just... hard to reach.</a>
        </p>
    </div>
    <!-- <details close>
        <summary> <h2 style="display: inline;">Why "noyii"</h2> </summary>
        <p>
        The name <strong>noyii</strong> is a small, personal echo of  Noÿs from Isaac Asimov’s 
        <em>The End of Eternity</em>. In the final chapters of the novel, Noys reveals what
        Eternity’s obsession with “optimizing” history really does to humanity: by surgically
        removing risk and deviation, it also removes the branches on which any ambitious future
        could grow.
    </p>

    <p>
        In the novel, Noÿs explains that while humanity remained safely on Earth for 125,000 Centuries, younger civilizations had caught up, passed them, and <strong>colonized the entire Galaxy</strong>. When humanity finally looked outward, they found the stars barred against them. 
    </p>

    <blockquote style="border-left: 4px solid #ccc; margin: 1.5em 10px; padding: 0.5em 10px; background-color: #f9f9f9;">
        “When we moved out into space, the signs were up. <em>Occupied! No Trespassing! Clear Out!</em> Mankind drew back its exploratory feelers, remained at home. But now he knew Earth for what it was: a prison surrounded by an infinity of freedom . . . And mankind died out!”
        <br>
        <footer style="font-size: 0.8em; text-align: right; color: #555;">
            Excerpt From: Isaac Asimov. “The End of Eternity.”
        </footer>
        <div style="clear:both;"></div>
    </blockquote>

    <p>
        It wasn't a sudden cataclysm, but a slow suffocation of the soul caused by the lack of a frontier. Noÿs describes the ultimate fate of this "safe" humanity:
    </p>

    <blockquote style="border-left: 4px solid #ccc; margin: 1.5em 10px; padding: 0.5em 10px; background-color: #f9f9f9;">
        “They didn’t <em>just</em> die out. It took thousands of Centuries. There were ups and downs but, on the whole, there was a loss of purpose, a sense of futility, a feeling of hopelessness that could not be overcome. Eventually there was one last decline of the birth rate and finally, extinction. Your Eternity did that.”
        <br>
        <footer style="font-size: 0.8em; text-align: right; color: #555;">
            Excerpt From: Isaac Asimov. “The End of Eternity.”
        </footer>
        <div style="clear:both;"></div>
    </blockquote>

    <p>
        <strong>noyii</strong> stands for the alternative. It aligns with Musk's philosophy that we must take the risks now to ensure we are the "first true colonists" later.
    </p>

    <p>
        That is why, for a long time, I have anchored my <a href="https://github.com/Noyii"><code>readme.md</code></a> with these two defining citations as a testament to the timeline I choose to inhabit:
    </p>

    <blockquote style="border-left: 4px solid #ccc; margin: 1.5em 10px; padding: 0.5em 10px; background-color: #f9f9f9;">
        <p style="font-style: italic; margin-bottom: 0.5em;">
            “With that disappearance, he knew, even as Noÿs moved slowly into his arms, came the end, the final end of Eternity.
            <br><br>
            – And the beginning of Infinity.”
        </p>
        <footer style="font-size: 0.8em; text-align: right; color: #555;">
            Excerpt From: Isaac Asimov. “The End of Eternity.”
        </footer>
    </blockquote>

    <blockquote style="border-left: 4px solid #ccc; margin: 1.5em 10px; padding: 0.5em 10px; background-color: #f9f9f9;">
        <p style="font-style: italic; margin-bottom: 0.5em;">
            “In the course of things children will be born, and families raised on Mars—the first true colonists of a new branch of human civilization.”
        </p>
        <footer style="font-size: 0.8em; text-align: right; color: #555;">
            Excerpt From: Zubrin, Robert. “The Case for Mars.”
        </footer>
    </blockquote>

    </details>
 -->


    <details close>
        <summary>
            <h2 style="display: inline;">News</h2>
        </summary>
        <ul>
            <li> <b>2026-01</b> &nbsp; One papers accepted by <em>ICLR 2026</em>, see you in Rio de Janeiro, Brazil! </li>
        </ul>
        <ul>
            <li> <b>2025-09</b> &nbsp; Two papers accepted by <em>NeurIPS 2025</em>. </li>
        </ul>
        <ul>
            <li> <b>2025-06</b> &nbsp; Two papers accepted by <em>ICCV 2025</em>. </li>
        </ul>
        <ul>
            <li> <b>2025-04</b> &nbsp; One paper accepted by <em>CVPR 2025 AI for Content Creation Workshop</em>. </li>
        </ul>
        <ul>
            <li> <b>2025-04</b> &nbsp; One paper accepted by <em>IJCAI 2025</em>. </li>
        </ul>
        <ul>
            <li> <b>2025-04</b> &nbsp; Invited Talk from Riot Video Games. </li>
        </ul>
        <ul>
            <li> <b>2025-03</b> &nbsp; Awarded <em>Frontier Top Ten Young Scholars Award</em> (1st) from Century
                Frontier Asset Management. </li>
        </ul>
        <ul>
            <li> <b>2025-03</b> &nbsp; Invited Talk from University of Illinois Urbana-Champaign (UIUC). </li>
        </ul>
        <ul>
            <li> <b>2025-02</b> &nbsp; One paper accepted by <em>CVPR 2025</em>. </li>
        </ul>
        <ul>
            <li> <b>2025-01</b> &nbsp; One paper accepted by <em>ICLR 2025</em>, see you in Singapore! </li>
        </ul>
        <ul>
            <li><b>2024-12</b> &nbsp; One paper accepted by <em>AAAI 2025</em>. </li>
        </ul>
        <ul>
            <li> <b>2024-11</b> &nbsp; Invited Talk from Safe SuperIntelligence (SSI) Club. </li>
        </ul>
        <ul>
            <li> <b>2024-04</b> &nbsp; One paper accepted by <em>IJCAI 2024</em>, see you in Jeju! </li>
        </ul>
        <ul>
            <li> <b>2023-08</b> &nbsp; One paper accepted by <em>BMVC 2023</em>. </li>
        </ul>
        <ul>
            <li> <b>2023-07</b> &nbsp; Two papers accepted by <em>ACM MM 2023</em>. </li>
        </ul>
        <ul>
            <li> <b>2023-07</b> &nbsp; Two papers accepted by <em>ICCV 2023</em>. </li>
        </ul>
        <ul>
            <li> <b>2023-06</b> &nbsp; <em>Taming Diffusion Models for Music-driven Conducting Motion
                    Generation</em> accepted by <em>AAAI 2023 Summer Symposium</em>, with <b>Best Paper
                    Award</b>. </li>
        </ul>
        <ul>
            <li> <b>2023-05</b> &nbsp; One paper accepted by <em>ICIP 2023</em>, see you in Kuala Lumpur! </li>
        </ul>
        <ul>
            <li> <b>2023-02</b> &nbsp; <em>Translating natural language to planning goals with
                    large-language models</em> now on arxiv. </li>
        </ul>
        <ul>
            <li> <b>2022-11</b> &nbsp; One paper accepted by <em>ACCV 2022</em>. </li>
        </ul>
        <ul>
            <li> <b>2022-06</b> &nbsp; <em>LaT: Latent Translation with Cycle-Consistency for Video-Text
                    Retrieval</em> now on arxiv. </li>
        </ul>

        <ul>
            <li> <b>2021-03</b> &nbsp; Awarded as <em>Outstanding Graduate</em> by <em>Nanjing
                    University</em>. </li>
        </ul>
        <ul>
            <li> <b>2019-03</b> &nbsp; Awarded as <em>Outstanding Student</em> by <em>Nanjing
                    University</em>. </li>
        </ul>
    </details>

    <!-- <details close>
                        <summary>
                            <h2 style="display: inline;">Professional Services</h2>
                        </summary>

                        <ul>
                            <li> Reviewer:&nbsp; <em>ECCV 2022</em>, <em>ACCV 2022</em>, <em>CVPR 2023</em>, <em>ICCV
                                    2023</em>, <em>ACM MM 2023</em>, <em>EMNLP 2023</em>, <em>ICASSP 2024</em>, <em>CVPR
                                    2024</em>, <em>ICPR 2024</em>, <em>ECCV 2024</em>, <em>ACM MM 2024</em>, <em>COLM 2024</em>, <em>NeurIPS 2024</em>, <em>Artificial Intelligence Review</em>, <em>ICASSP 2025</em>, <em>ICLR 2025</em>, <em>AISTATS 2025</em>, <em>CVPR 2025</em>, <em>ICML 2025</em>, <em>IJCNN 2025</em>, <em>COLM 2025</em>, <em>ICCV 2025</em>, <em>NeurIPS 2025</em>, <em>AI for Content Creation workshop at CVPR 2025</em>    
                        </ul>
                        <ul>
                            <li> Program Committee Member:&nbsp; <em>AAAI 2023</em>, <em>AAAI 2024</em>, <em>AAAI 2025</em>
                        </ul>
                        <ul>
                            <li> ACM Professional Member</li>
                        </ul>
                    </details> -->

    <h2>Selected Publications (See <a href="https://scholar.google.com/citations?hl=en&user=PAfNNrYAAAAJ">Google Scholar</a> for full list)</h2> 

    <!-- <a href="pub.html">Full List</a> -->

     <table
                        style="width: 100%; margin-left:auto;margin-right:auto; border-width: 0px; border-spacing: 0px;">

                        <tbody>
                            <tr style="border-width: 1px">
                                <td style="border: 0px; padding: 20px">
                                    <img src="paper/Prism.png" alt="guide"
                                       style=" width: 100px; margin-top: 8px">
                                </td>
                                <td style="border: 0px; padding: 20px">
                                    <div class="table-responsive">
                                        <table style="width: 100%;" class="table">
                                            <tbody>
                                                <tr>
                                                    <td style="width: 100%; text-align: left; border: 0px">
                                                        <p>
                                                            <br>
                                                            <b>Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models</b>
                                                            <br>
                                                            <strong>Jinbin Bai</strong>, Yixuan Li, Yuchen Zhu, Yi Xin, Qingyu Shi, Aosong Feng, Xiaohong Liu, Molei Tao, Jianru Xue, Xiangtai Li, Ming-Hsuan Yang 
                                                            <br>
                                                            Technical Report 2026
                                                            <br>
                                                            <a href="https://arxiv.org/abs/">[Paper]</a>
                                                            <a
                                                            href="https://github.com/viiika/Prism">[GitHub]</a>
                                                            <a
                                                            href="">[Media_Report_CN]</a>
                                                            

                                                            
                                                            <br>
                                                            An efficient test-time-scaling method for discrete diffusion language models to unlock dLLMs' full generative potential. Experiments are on LLaDA, Dream, LLaDA2.
                                                            <br>
                                                        </p>
                                                    </td>
                                                    <td style="width: 10px;border: 0px">
                                                    </td>
                                                </tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </td>
                            </tr>
                        </tbody>

                    </table>

    <table
                        style="width: 100%; margin-left:auto;margin-right:auto; border-width: 0px; border-spacing: 0px;">

                        <tbody>
                            <tr style="border-width: 1px">
                                <td style="border: 0px; padding: 20px">
                                    <img src="paper/guide.png" alt="guide"
                                       style=" width: 100px; margin-top: 8px">
                                </td>
                                <td style="border: 0px; padding: 20px">
                                    <div class="table-responsive">
                                        <table style="width: 100%;" class="table">
                                            <tbody>
                                                <tr>
                                                    <td style="width: 100%; text-align: left; border: 0px">
                                                        <p>
                                                            <br>
                                                            <b>From Masks to Worlds: A Hitchhiker’s Guide to World Models</b>
                                                            <br>
                                                            <strong>Jinbin Bai</strong>, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang
                                                            <br>
                                                            Technical Report 2025
                                                            <br>
                                                            <a href="https://arxiv.org/abs/2510.20668">[Paper]</a>
                                                            <a
                                                            href="https://github.com/M-E-AGI-Lab/Awesome-World-Models">[GitHub]</a>
                                                            <a
                                                            href="https://mp.weixin.qq.com/s/i2HNtAZrdgSe5-cM9KHEJg">[Media_Report_CN]</a>
                                                            <a href="https://www.youtube.com/watch?v=07NQQRs2jFM">[YouTube_EN]</a>
                                                            <a href="https://www.youtube.com/watch?v=ZfReAetf8xk">[YouTube_KO]</a>

                                                            
                                                            <br>
                                                            A Hitchhiker’s guide for those who want to build worlds. We follow one clear road: from early masked models, to unified architectures that share a single paradigm, then to interactive generative models, and finally to memory-augmented systems that sustain consistent worlds over time. 
                                                            <br>
                                                        </p>
                                                    </td>
                                                    <td style="width: 10px;border: 0px">
                                                    </td>
                                                </tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </td>
                            </tr>
                        </tbody>

                    </table>

    <table style="width: 100%; margin-left:auto;margin-right:auto; border-width: 0px; border-spacing: 0px;">

        <tbody>
            <tr style="border-width: 1px">
                <td style="border: 0px; padding: 20px">
                    <img src="paper/DiMOO.png" alt="DiMOO" style=" width: 100px; margin-top: 8px">
                </td>
                <td style="border: 0px; padding: 20px">
                    <div class="table-responsive">
                        <table style="width: 100%;" class="table">
                            <tbody>
                                <tr>
                                    <td style="width: 100%; text-align: left; border: 0px">
                                        <p>
                                            <br>
                                            <b>Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal
                                                Generation and Understanding</b>
                                            <br>
                                            Yi Xin, ..., <strong>Jinbin Bai</strong>, ... (Alpha VLLM Team)
                                            <br>
                                            Technical Report 2025
                                            <br>
                                            <a href="https://arxiv.org/abs/2510.06308">[Paper]</a>
                                            <a href="https://huggingface.co/Alpha-VLLM/Lumina-DiMOO">[Model]</a>
                                            <a href="https://github.com/Alpha-VLLM/Lumina-DiMOO">[Code]</a>


                                            <br>
                                            Lumina-DiMOO is a unified masked diffusion model that can not only
                                            generate high-resolution images, but also support multimodal capabilities
                                            including text-to-image, image-to-image, and image understanding. SOTA performance with novel application <b>Interactive Retouching</b>!
                                            <br>
                                        </p>
                                    </td>
                                    <td style="width: 10px;border: 0px">
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </td>
            </tr>
        </tbody>

    </table>

    <table style="width: 100%; margin-left:auto;margin-right:auto; border-width: 0px; border-spacing: 0px;">

        <tbody>
            <tr style="border-width: 1px">
                <td style="border: 0px; padding: 20px">
                    <img src="paper/Evolution.png" alt="Muddit" style=" width: 100px; margin-top: 8px">
                </td>
                <td style="border: 0px; padding: 20px">
                    <div class="table-responsive">
                        <table style="width: 100%;" class="table">
                            <tbody>
                                <tr>
                                    <td style="width: 100%; text-align: left; border: 0px">
                                        <p>
                                            <br>
                                            <b>Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model</b>
                                            <br>
                                            Qingyu Shi*, <strong>Jinbin Bai*</strong>, Zhuoran Zhao, ..., Shuicheng Yan  (* denotes equal contribution)
                                            <br>
                                            ICLR 2026
                                            <br>
                                            <a href="https://arxiv.org/abs/2505.23606">[Paper]</a>
                                            <a href="https://huggingface.co/MeissonFlow/Muddit">[Model]</a>
                                            <a href="https://github.com/M-E-AGI-Lab/Muddit">[Code]</a>


                                            <br>
                                            Muddit (offical Meissonic II) is a unified masked diffusion model that can not only
                                            generate high-resolution images, but support multimodal capabilities
                                            including text-to-image, image-to-text, and VQA. We verified one unified model can be trained from visual prior learned by Meissonic!
                                            <br>
                                        </p>
                                    </td>
                                    <td style="width: 10px;border: 0px">
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </td>
            </tr>
        </tbody>

    </table>


    <table style="width: 100%; margin-left:auto;margin-right:auto; border-width: 0px; border-spacing: 0px;">

        <tbody>
            <tr style="border-width: 1px">
                <td style="border: 0px; padding: 20px">
                    <img src="paper/Meissonic.png" alt="Meissonic" style="height: 100px; width: 100px; margin-top: 8px">
                </td>
                <td style="border: 0px; padding: 20px">
                    <div class="table-responsive">
                        <table style="width: 100%;" class="table">
                            <tbody>
                                <tr>
                                    <td style="width: 100%; text-align: left; border: 0px">
                                        <p>
                                            <br>
                                            <b>Meissonic: Revitalizing Masked Generative Transformers for Efficient
                                                High-Resolution Text-to-Image Synthesis</b>
                                            <br>
                                            <strong>Jinbin Bai</strong>, ..., Shuicheng Yan 
                                            <br>
                                            ICLR 2025
                                            <br>
                                            <a href="https://arxiv.org/abs/2410.08261">[Paper]</a>
                                            <a href="https://huggingface.co/MeissonFlow/Meissonic">[Model]</a>
                                            <a href="https://github.com/viiika/Meissonic">[Code]</a>
                                            <a href="https://huggingface.co/spaces/MeissonFlow/meissonic">[Demo]</a>
                                            <a
                                                href="https://discord.com/channels/1305165461192966166/1305165622258303016">[Discord_Discussion]</a>
                                            <a href="https://www.youtube.com/watch?v=PlmifElhr6M">[Toturial_EN]</a>
                                            <a href="https://www.youtube.com/watch?v=rJDrf49wF64">[Toturial_JA]</a>
                                            <a
                                                href="https://mp.weixin.qq.com/s/0FULkgOeAyLnwR-u4AL1-A">[Media_Report_CN1]</a>
                                            <a
                                                href="https://mp.weixin.qq.com/s/pYZxK3OFV8CH4VQET4_rLg">[Media_Report_CN2]</a>

                                            <br>
                                            Meissonic is a text-to-image masked diffusion model that can generate
                                            high-resolution images. It is designed to run on consumer graphics cards.
                                            The left figure is generated by Meissonic.
                                            <br>
                                        </p>
                                    </td>
                                    <td style="width: 10px;border: 0px">
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </td>
            </tr>
        </tbody>

    </table>

    <details open>
        <summary>
            <h2 style="display: inline;">Miscellaneous</h2>
        </summary>
        <ul>
            <li>I am a huge fan of Cities: Skylines and I love designing and simulating cities. I can't
                wait for the release of Cities: Skylines II on Oct 24th, 2023! And, I've attended World Cities Summit
                (WCS) 2024 Conference!</li>
            <li>My favorite movies in recent years is Free Guy, and I dream of designing a game like
                this.</li>
            <!-- <li>In my leisure moments, I delight in playing the piano surrounded by abundant greenery, finding peace and
                emotional comfort in the solitude.</li> -->
            <li>I enjoy traveling and have visited 13 countries, guess where I have been?</li>
            <li>I like swimming, diving, surfing, beach under the sunshine.</li>
        </ul>
    </details>


    </td>
    </tr>
    <script id='clustrmaps'
        src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=Y9TH27aN8aIzbOU_6mOR3Oi70J95-HEqjFWoA2E0vYI&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>



    </tbody>

    </table>
</body>

</html>